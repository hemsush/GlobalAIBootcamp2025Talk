{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6eb7dc8-601d-4abf-a3d0-a3f79be8f3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Create, evaluate, and deploy an AI agent\n",
    "\n",
    "This notebook demonstrates how to use Mosaic AI to evaluate and improve the quality, cost, and latency of a tool-calling agent. It also shows you how to deploy the resulting agent to a web-based chat UI.\n",
    "\n",
    "Using Mosiac AI Agent Evaluation ([AWS](https://docs.databricks.com/en/generative-ai/agent-evaluation/index.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-evaluation/)), Agent Framework ([AWS](https://docs.databricks.com/en/generative-ai/agent-framework/build-genai-apps.html) |[Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/build-genai-apps)), MLflow ([AWS](https://docs.databricks.com/en/generative-ai/agent-framework/log-agent.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/log-agent)) and Model Serving ([AWS](https://docs.databricks.com/en/generative-ai/agent-framework/deploy-agent.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/deploy-agent)), this notebook:\n",
    "1. Generates synthetic evaluation data from a document corpus.\n",
    "2. Creates a tool-calling agent with a retriever tool.\n",
    "3. Evaluates the agent's quality, cost, and latency across several foundational models.\n",
    "4. Deploys the agent to a web-based chat app.\n",
    "\n",
    "## Requirements: \n",
    "* Use serverless compute or a cluster running Databricks Runtime 14.3 or above.\n",
    "* Databricks Serverless and Unity Catalog enabled.\n",
    "* CREATE MODEL access to a Unity Catalog schema.\n",
    "* Permission to create Model Serving endpoints.\n",
    "\n",
    "<img src=\"https://docs.databricks.com/_static/images/generative-ai/synth-evals/demo-overview-optimized.gif\"/>\n",
    "\n",
    "For videos that go deeper into the capabilities, see this [YouTube channel](https://www.youtube.com/@EricPeter-q6o).\n",
    "\n",
    "## Want to use your own data?\n",
    "\n",
    "Alternatively, if you already have a Databricks Vector Search index set up, you can use the version of this notebook designed to use your own data ([AWS](https://docs.databricks.com/generative-ai/tutorials/agent-framework-notebook.html) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/tutorials/agent-framework-notebook))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32b90384-83c4-4ae6-a3ee-87f17319c00c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71756c5a-8822-4023-a5d7-9795bb1d3236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq databricks-agents mlflow databricks-sdk[openai] backoff\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06447894-fae4-4fe6-ae63-cb1f8d26b176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d1e45b9-62ac-4e0d-aaca-76faff6247e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 1. Generate synthetic evaluation data to measure quality\n",
    "\n",
    "**Challenges addressed**\n",
    "1. How to start quality evaluation with diverse, representative data without SMEs spending months labeling?\n",
    "\n",
    "**What is happening?**\n",
    "- We pass the documents to the Synthetic API along with a `num_evals` and prompt-like `agent_description` and `question_guidelines` to tailor the generated questions for our use case. This API uses a proprietary synthetic generation pipeline developed by Mosaic AI Research.\n",
    "- The API produces `num_evals` questions, each coupled with the source document and a list of facts, generated based on the source document. Each fact must be present in the agent's response for it to be considered correct.\n",
    "\n",
    "*Why does the the API generates a list of facts, rather than a fully written answer. This...*\n",
    "- Makes SME review more efficient: by focusing on facts rather than a full response, they can review and edit more quickly.\n",
    "- Improves the accuracy of our proprietary LLM judges.\n",
    "\n",
    "Interested in have your SMEs review the data? Check out a [video demo of the Eval Set UI](https://youtu.be/avY9724q4e4?feature=shared&t=130)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bab4b34-29c1-4569-984b-da2736b0ffc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Load the docs corpus\n",
    "First, load the documents (Databricks documentation) used by the agent, filtering for a subset of the documentation.\n",
    "\n",
    "For your agent, replace this step to instead load your parsed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ea7676-260e-463a-a518-fc41418af29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "databricks_docs_url = \"https://raw.githubusercontent.com/databricks/genai-cookbook/refs/heads/main/quick_start_demo/chunked_databricks_docs_filtered.jsonl\"\n",
    "parsed_docs_df = pd.read_json(databricks_docs_url, lines=True)\n",
    "\n",
    "display(parsed_docs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f38f0645-aa20-4ad1-885c-b2f02c1b2c04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Call API to generate synthetic evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7264aaa2-360e-4b12-af63-df1ab111644c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the synthetic eval generation API to get some evals\n",
    "from databricks.agents.evals import generate_evals_df\n",
    "\n",
    "# \"Ghost text\" for agent description and question guidelines - feel free to modify as you see fit.\n",
    "agent_description = f\"\"\"\n",
    "The agent is a RAG chatbot that answers questions about Databricks. Questions unrelated to Databricks are irrelevant.\n",
    "\"\"\"\n",
    "question_guidelines = f\"\"\"\n",
    "# User personas\n",
    "- A developer who is new to the Databricks platform\n",
    "- An experienced, highly technical Data Scientist or Data Engineer\n",
    "\n",
    "# Example questions\n",
    "- what API lets me parallelize operations over rows of a delta table?\n",
    "- Which cluster settings will give me the best performance when using Spark?\n",
    "\n",
    "# Additional Guidelines\n",
    "- Questions should be succinct, and human-like\n",
    "\"\"\"\n",
    "\n",
    "num_evals = 25\n",
    "evals = generate_evals_df(\n",
    "    docs=parsed_docs_df[\n",
    "        :500\n",
    "    ],  # Pass your docs. They should be in a Pandas or Spark DataFrame with columns `content STRING` and `doc_uri STRING`.\n",
    "    num_evals=num_evals,  # How many synthetic evaluations to generate\n",
    "    agent_description=agent_description,\n",
    "    question_guidelines=question_guidelines,\n",
    ")\n",
    "display(evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aca6a1a-17a2-4c89-b8ce-ce54214ed0e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 2. Write the agent's code\n",
    "\n",
    "### Function-calling agent that uses a retriever tool\n",
    "\n",
    "**Challenges addressed**\n",
    "- How do I track different versions of my agent's code or configuration?\n",
    "- How do I enable observability, monitoring, and debugging of my agent’s logic?\n",
    "\n",
    "**What is happening?**\n",
    "\n",
    "First, create a function-calling agent with access to a retriever tool using OpenAI SDK and Python code. To keep the demo simple, the retriever is a function that performs keyword lookup rather than a vector search index.\n",
    "\n",
    "When creating your agent, you can either:\n",
    "1. Generate template agent code from the AI Playground\n",
    "2. Use a template from our Cookbook\n",
    "3. Start from an example in popular frameworks such as LangGraph, AutoGen, LlamaIndex, and others.\n",
    "\n",
    "**NOTE: It is not necessary to understand how this agent works to understand the rest of this demo notebook.**  \n",
    "\n",
    "*A few things to note about the code:*\n",
    "1. The code is written to `fc_agent.py` in order to use [MLflow Models from Code](https://www.mlflow.org/blog/models_from_code) for logging, enabling easy tracking of each iteration as you tune the agent for quality.\n",
    "2. The code is parameterized with an MLflow Model Configuration ([AWS](https://docs.databricks.com/en/generative-ai/agent-framework/create-agent.html#use-parameters-to-configure-the-agent) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/create-agent#agent-parameters)), enabling easy tuning of these parameters for quality improvement.\n",
    "3. The code is wrapped in an MLflow [ChatModel](https://mlflow.org/docs/latest/llms/chat-model-intro/index.html), making the agent's code deployment-ready so any iteration can be shared with stakeholders for testing.\n",
    "4. The code implements MLflow Tracing ([AWS](https://docs.databricks.com/en/mlflow/mlflow-tracing.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/mlflow/mlflow-tracing)) for unified observability during development and production. The same trace defined here will be logged for every production request post-deployment. For agent authoring frameworks like LangChain and LlamaIndex, you can perform tracing with one line of code: `mlflow.langchain.autolog()` or `mlflow.llama_index.autolog()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9783d93-23a4-4bca-bc7a-569d1028a665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile fc_agent.py\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import pandas as pd\n",
    "from typing import Any, Union, Dict, List, Optional\n",
    "import mlflow\n",
    "from mlflow.pyfunc import ChatModel\n",
    "from mlflow.types.llm import ChatCompletionResponse, ChatMessage, ChatParams, ChatChoice\n",
    "from dataclasses import asdict\n",
    "import dataclasses\n",
    "import json\n",
    "import backoff  # for exponential backoff on LLM rate limits\n",
    "\n",
    "\n",
    "# Default configuration for the agent.\n",
    "DEFAULT_CONFIG = {\n",
    "    'endpoint_name': \"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    'temperature': 0.01,\n",
    "    'max_tokens': 1000,\n",
    "    'system_prompt': \"\"\"You are a helpful assistant that answers questions about Databricks. Questions unrelated to Databricks are irrelevant.\n",
    "\n",
    "    You answer questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "    \"\"\",\n",
    "    'max_context_chars': 4096 * 4\n",
    "}\n",
    "\n",
    "# OpenAI-formatted function for the retriever tool\n",
    "RETRIEVER_TOOL_SPEC = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"search_product_docs\",\n",
    "        \"description\": \"Use this tool to search for Databricks product documentation.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False,\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"description\": \"a set of individual keywords to find relevant docs for. each item of the array must be a single word.\",\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}]\n",
    "\n",
    "class FunctionCallingAgent(mlflow.pyfunc.ChatModel):\n",
    "    \"\"\"\n",
    "    Class representing a function-calling agent that has one tool: a retriever using keyword-based search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the OpenAI SDK client connected to Model Serving.\n",
    "        Load the agent's configuration from MLflow Model Config.\n",
    "        \"\"\"\n",
    "        # Initialize OpenAI SDK connected to Model Serving\n",
    "        w = WorkspaceClient()\n",
    "        self.model_serving_client: OpenAI = w.serving_endpoints.get_open_ai_client()\n",
    "\n",
    "        # Load config\n",
    "        # When this agent is deployed to Model Serving, the configuration loaded here is replaced with the config passed to mlflow.pyfunc.log_model(model_config=...)\n",
    "        self.config = mlflow.models.ModelConfig(development_config=DEFAULT_CONFIG)\n",
    "\n",
    "        # Configure playground, review app, and agent evaluation to display the chunks from the retriever \n",
    "        mlflow.models.set_retriever_schema(\n",
    "            name=\"db_docs\",\n",
    "            primary_key=\"chunk_id\",\n",
    "            text_column=\"chunked_text\",\n",
    "            doc_uri=\"doc_uri\",\n",
    "        )\n",
    "\n",
    "        # Load the retriever tool's docs.\n",
    "        raw_docs_parquet = \"https://github.com/databricks/genai-cookbook/raw/refs/heads/main/quick_start_demo/chunked_databricks_docs.snappy.parquet\"\n",
    "        self.docs = pd.read_parquet(raw_docs_parquet).to_dict(\"records\")\n",
    "\n",
    "        # Identify the function used as the retriever tool\n",
    "        self.tool_functions = {\n",
    "            'search_product_docs': self.search_product_docs\n",
    "        }\n",
    "\n",
    "    @mlflow.trace(name=\"rag_agent\", span_type=\"AGENT\")\n",
    "    def predict(\n",
    "        self, context=None, messages: List[ChatMessage]=None, params: Optional[ChatParams] = None\n",
    "    ) -> ChatCompletionResponse:\n",
    "        \"\"\"\n",
    "        Primary function that takes a user's request and generates a response.\n",
    "        \"\"\"\n",
    "        if messages is None:\n",
    "            raise ValueError(\"predict(...) called without `messages` parameter.\")\n",
    "        \n",
    "        # Convert all input messages to dict from ChatMessage\n",
    "        messages = convert_chat_messages_to_dict(messages)\n",
    "\n",
    "        # Add system prompt\n",
    "        request = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": self.config.get('system_prompt')},\n",
    "                    *messages,\n",
    "                ],\n",
    "            }\n",
    "            \n",
    "        # Ask the LLM to call tools and generate the response\n",
    "        output= self.recursively_call_and_run_tools(\n",
    "            **request\n",
    "        )\n",
    "        \n",
    "        # Convert response to ChatCompletionResponse dataclass\n",
    "        return ChatCompletionResponse.from_dict(output)\n",
    "    \n",
    "    @mlflow.trace(span_type=\"RETRIEVER\")\n",
    "    def search_product_docs(self, query: list[str]) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Retriever tool. Simple keyword-based retriever - would be replaced with a Vector Index\n",
    "        \"\"\"\n",
    "        keywords = query\n",
    "        if len(keywords) == 0:\n",
    "            return []\n",
    "        result = []\n",
    "        for chunk in self.docs:\n",
    "            score = sum(\n",
    "                (keyword.lower() in chunk[\"chunked_text\"].lower())\n",
    "                for keyword in keywords\n",
    "            )\n",
    "            result.append(\n",
    "                {\n",
    "                    \"page_content\": chunk[\"chunked_text\"],\n",
    "                    \"metadata\": {\n",
    "                        \"doc_uri\": chunk[\"url\"],\n",
    "                        \"score\": score,\n",
    "                        \"chunk_id\": chunk[\"chunk_id\"],\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "        ranked_docs = sorted(result, key=lambda x: x[\"metadata\"][\"score\"], reverse=True)\n",
    "        cutoff_docs = []\n",
    "        context_budget_left = self.config.get(\"max_context_chars\")\n",
    "        for doc in ranked_docs:\n",
    "            content = doc[\"page_content\"]\n",
    "            doc_len = len(content)\n",
    "            if context_budget_left < doc_len:\n",
    "                cutoff_docs.append(\n",
    "                    {**doc, \"page_content\": content[:context_budget_left]}\n",
    "                )\n",
    "                break\n",
    "            else:\n",
    "                cutoff_docs.append(doc)\n",
    "            context_budget_left -= doc_len\n",
    "        return cutoff_docs\n",
    "\n",
    "    ##\n",
    "    # Helper functions below\n",
    "    ##\n",
    "    @backoff.on_exception(backoff.expo, openai.RateLimitError)\n",
    "    def completions_with_backoff(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Helper: exponetially backoff if the LLM's rate limit is exceeded.\n",
    "        \"\"\"\n",
    "        traced_chat_completions_create_fn = mlflow.trace(\n",
    "            self.model_serving_client.chat.completions.create,\n",
    "            name=\"chat_completions_api\",\n",
    "            span_type=\"CHAT_MODEL\",\n",
    "        )\n",
    "        return traced_chat_completions_create_fn(**kwargs)\n",
    "\n",
    "    def chat_completion(self, messages: List[ChatMessage]) -> ChatCompletionResponse:\n",
    "        \"\"\"\n",
    "        Helper: Call the LLM configured via the ModelConfig using the OpenAI SDK\n",
    "        \"\"\"\n",
    "        request = {\"messages\": messages, \"temperature\": self.config.get(\"temperature\"), \"max_tokens\": self.config.get(\"max_tokens\"),  \"tools\": RETRIEVER_TOOL_SPEC}\n",
    "        return self.completions_with_backoff(\n",
    "            model=self.config.get(\"endpoint_name\"), **request,\n",
    "                \n",
    "        )\n",
    "\n",
    "    @mlflow.trace(span_type=\"CHAIN\")\n",
    "    def recursively_call_and_run_tools(self, max_iter=10, **kwargs):\n",
    "        \"\"\"\n",
    "        Helper: Recursively calls the LLM using the tools in the prompt. Either executes the tools and recalls the LLM or returns the LLM's generation.\n",
    "        \"\"\"\n",
    "        messages = kwargs[\"messages\"]\n",
    "        del kwargs[\"messages\"]\n",
    "        i = 0\n",
    "        while i < max_iter:\n",
    "            with mlflow.start_span(name=f\"iteration_{i}\", span_type=\"CHAIN\") as span:\n",
    "                response = self.chat_completion(messages=messages)\n",
    "                assistant_message = response.choices[0].message  # openai client\n",
    "                tool_calls = assistant_message.tool_calls  # openai\n",
    "                if tool_calls is None:\n",
    "                    # the tool execution finished, and we have a generation\n",
    "                    return response.to_dict()\n",
    "                tool_messages = []\n",
    "                for tool_call in tool_calls:  # TODO: should run in parallel\n",
    "                    with mlflow.start_span(\n",
    "                        name=\"execute_tool\", span_type=\"TOOL\"\n",
    "                    ) as span:\n",
    "                        function = tool_call.function  \n",
    "                        args = json.loads(function.arguments)  \n",
    "                        span.set_inputs(\n",
    "                            {\n",
    "                                \"function_name\": function.name,\n",
    "                                \"function_args_raw\": function.arguments,\n",
    "                                \"function_args_loaded\": args,\n",
    "                            }\n",
    "                        )\n",
    "                        result = self.execute_function(\n",
    "                            self.tool_functions[function.name], args\n",
    "                        )\n",
    "                        tool_message = {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"content\": result,\n",
    "                        } \n",
    "\n",
    "                        tool_messages.append(tool_message)\n",
    "                        span.set_outputs({\"new_message\": tool_message})\n",
    "                assistant_message_dict = assistant_message.dict().copy()  \n",
    "                del assistant_message_dict[\"content\"]\n",
    "                del assistant_message_dict[\"function_call\"] \n",
    "                if \"audio\" in assistant_message_dict:\n",
    "                    del assistant_message_dict[\"audio\"]  # hack to make llama70b work\n",
    "                messages = (\n",
    "                    messages\n",
    "                    + [\n",
    "                        assistant_message_dict,\n",
    "                    ]\n",
    "                    + tool_messages\n",
    "                )\n",
    "                i += 1\n",
    "        # TODO: Handle more gracefully\n",
    "        raise \"ERROR: max iter reached\"\n",
    "\n",
    "    def execute_function(self, tool, args):\n",
    "        \"\"\"\n",
    "        Execute a tool and return the result as a JSON string\n",
    "        \"\"\"\n",
    "        result = tool(**args)\n",
    "        return json.dumps(result)\n",
    "        \n",
    "def convert_chat_messages_to_dict(messages: List[ChatMessage]):\n",
    "    new_messages = []\n",
    "    for message in messages:\n",
    "        if type(message) == ChatMessage:\n",
    "            # Remove any keys with None values\n",
    "            new_messages.append({k: v for k, v in asdict(message).items() if v is not None})\n",
    "        else:\n",
    "            new_messages.append(message)\n",
    "    return new_messages\n",
    "    \n",
    "\n",
    "# tell MLflow logging where to find the agent's code\n",
    "mlflow.models.set_model(FunctionCallingAgent())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c46e61-43ae-486d-b092-971f0ae96a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Empty `__init__.py` to allow the `FunctionCallingAgent()` to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1e24df-73cd-4345-927d-2435e2ae845b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile __init__.py\n",
    "\n",
    "# Empty file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfd0d8be-efdf-416e-8324-f2af18fe1c8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Vibe check the agent\n",
    "\n",
    "Test the agent for a sample query to see the MLflow Trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e433dd92-e487-4d0d-a614-be6107dbcb73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import fc_agent\n",
    "from fc_agent import FunctionCallingAgent\n",
    "fc_agent = FunctionCallingAgent()\n",
    "\n",
    "response = fc_agent.predict(messages=[{\"role\": \"user\", \"content\": \"What is lakehouse monitoring?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d6e7b85-2f97-4e7a-8e8b-f89731872027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 3. Evaluate the agent\n",
    "\n",
    "## Initial evaluation\n",
    "\n",
    "**Challenges addressed**\n",
    "- What are the right metrics to evaluate quality? How do I trust the outputs of these metrics?\n",
    "- I need to evaluate many ideas - how do I…\n",
    "    - …run evaluation quickly so the majority of my time isn’t spent waiting?\n",
    "    - …quickly compare these different versions of my agent on quality, cost, and latency?\n",
    "- How do I quickly identify the root cause of any quality problems?\n",
    "\n",
    "**What is happening?**\n",
    "\n",
    "Now, run Agent Evaluation's proprietary LLM judges using the synthetic evaluation set to see the quality, cost, and latency of the agent and identify any root causes of quality issues. Agent Evaluation is tightly integrated with `mlflow.evaluate()`. \n",
    "\n",
    "Mosaic AI Research has invested signficantly in the quality AND speed of the LLM judges, optimizing the judges to agree with human raters. Read more [details in our blog](https://www.databricks.com/blog/databricks-announces-significant-improvements-built-llm-judges-agent-evaluation) about how our judges outperform the competition. \n",
    "\n",
    "After evaluation runs, click `View Evaluation Results` to open the MLflow UI for this Run. This lets you:\n",
    "- See summary metrics\n",
    "- See root cause analysis that identifies the most important issues to fix\n",
    "- Inspect individual responses to gain intuition about how the agent is performing\n",
    "- See the judge outputs to understand why the responses were graded as pass or fail\n",
    "- Compare between multiple runs to see how quality changed between experiments\n",
    "\n",
    "You can also inspect the other tabs:\n",
    "- `Overview` lets you see the agent's configuration and parameters\n",
    "- `Artifacts` lets you see the agent's code\n",
    "\n",
    "This UIs, coupled with the speed of evaluation, help you efficiently test your hypotheses to improve quality, letting you reach the production quality bar in less time. \n",
    "\n",
    "<img src=\"https://docs.databricks.com/_static/images/generative-ai/synth-evals/eval-1-optimized.gif\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e454e8-94fe-4315-8d94-2b88d98cfde2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.resources import DatabricksServingEndpoint\n",
    "import mlflow\n",
    "\n",
    "# First, define a helper function so you can compare the agent across multiple parameters and LLMs.\n",
    "def log_and_evaluate_agent(agent_config: dict, run_name: str):\n",
    "\n",
    "    # Define the databricks resources so this logged agent is deployment ready\n",
    "    resources = [DatabricksServingEndpoint(endpoint_name=agent_config[\"endpoint_name\"])]\n",
    "\n",
    "    # Start a run to contain the agent. `run_name` is a human-readable label for this run.\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log the agent's code and configuration to MLflow\n",
    "        model_info = mlflow.pyfunc.log_model(\n",
    "            python_model=\"fc_agent.py\",\n",
    "            artifact_path=\"agent\",\n",
    "            model_config=agent_config,\n",
    "            resources=resources,\n",
    "            input_example={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"What is lakehouse monitoring?\"}\n",
    "                ]\n",
    "            },\n",
    "            pip_requirements=[\"databricks-sdk[openai]\", \"mlflow\", \"databricks-agents\", \"backoff\"],\n",
    "        )\n",
    "\n",
    "        # Run evaluation\n",
    "        eval_results = mlflow.evaluate(\n",
    "            data=evals,  # Your evaluation set\n",
    "            model=model_info.model_uri,  # Logged agent from above\n",
    "            model_type=\"databricks-agent\",  # activate Mosaic AI Agent Evaluation\n",
    "        )\n",
    "\n",
    "        return (model_info, eval_results)\n",
    "\n",
    "\n",
    "# Now, call the helper function to run evaluation.\n",
    "# The configuration keys must match those defined in `fc_agent.py`\n",
    "model_info_llama_70b, eval_results = log_and_evaluate_agent(\n",
    "    agent_config={\n",
    "        \"endpoint_name\": \"databricks-meta-llama-3-1-70b-instruct\",\n",
    "        \"temperature\": 0.01,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"system_prompt\": \"\"\"You are a helpful assistant that answers questions about Databricks. Questions unrelated to Databricks are irrelevant.\n",
    "\n",
    "    You answer questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "    \"\"\",\n",
    "        \"max_context_chars\": 4096 * 4,\n",
    "    },\n",
    "    run_name=\"llama-3-1-70b-instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a4dbc3d-5179-47ba-adf6-5910b2512c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Compare multiple LLMs on quality, cost, and latency\n",
    "\n",
    "**Challenges addressed**\n",
    "- How to determine the foundational model that offers the right balance of quality, cost, and latency?\n",
    "\n",
    "**What is happening?**\n",
    "\n",
    "Normally, you would use the evaluation results to inform your hypotheses to improve quality, iteratively implementing, evaluating, and comparing each idea to the baseline. This demo assumes that you have fixed any root causes identified above and now want to optimize the agent for quality, cost, and latency. \n",
    "\n",
    "Here, you run evaluation for several LLMs. After the evaluation runs, click `View Evaluation Results` to open the MLflow UI for one of the runs. In the MLFLow Evaluations UI, use the **Compare to Run** dropdown to select another run name. This comparison view helps you quickly identify where the agent got better, worse, or stayed the same.\n",
    "\n",
    "Then, go to the MLflow Experiement page and click the chart icon in the upper left corner by `Runs`. Here, you can compare the models quantiatively across quality, cost, and latency metrics. The number of tokens used serves as a proxy for cost.\n",
    "\n",
    "This helps you make informed tradeoffs in partnership with your business stakeholders about quality, cost, and latency. Further, you can use this view to provide quantitative updates to your stakeholders so they can follow your progress improving quality.\n",
    "\n",
    "<img src=\"https://docs.databricks.com/_static/images/generative-ai/synth-evals/eval-2-optimized.gif\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04f68b5-efaf-446b-b33e-621856f6d8b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "baseline_config = {\n",
    "    \"endpoint_name\": \"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"system_prompt\": \"\"\"You are a helpful assistant that answers questions about Databricks. Questions unrelated to Databricks are irrelevant.\n",
    "\n",
    "    You answer questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "    \"\"\",\n",
    "    \"max_context_chars\": 4096 * 4,\n",
    "}\n",
    "\n",
    "llama405b_config = baseline_config.copy()\n",
    "llama405b_config[\"endpoint_name\"] = \"databricks-meta-llama-3-1-405b-instruct\"\n",
    "llama405b_config, _ = log_and_evaluate_agent(\n",
    "    agent_config=llama405b_config,\n",
    "    run_name=\"llama-3-1-405b-instruct\",\n",
    ")\n",
    "\n",
    "# If you have an External Model, such as OpenAI, uncomment this code, and replace `<my-external-model-endpoint-name>` to include this model in the evaluation\n",
    "# my_model_config = baseline_config.copy()\n",
    "# my_model_config['endpoint_name'] = '<my-external-model-endpoint-name>'\n",
    "\n",
    "# model_info_my_model_config, _ = log_and_evaluate_agent(\n",
    "#     agent_config=my_model_config,\n",
    "#     run_name=my_model_config['endpoint_name'],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdf7d5d7-e3b5-4798-ab8d-24834156e0a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Step 4. [Optional] Deploy the agent\n",
    "\n",
    "### Deploy to pre-production for stakeholder testing\n",
    "\n",
    "**Challenges addressed**\n",
    "- How do I quickly create a Chat UI for stakeholders to test the agent?\n",
    "- How do I track each piece of feedback and have it linked to what is happening in the bot so I can debug issues – without resorting to spreadsheets?\n",
    "\n",
    "**What is happening?**\n",
    "\n",
    "First, register one of the agent models that you logged above to Unity Catalog. Then, use Agent Framework to deploy the agent to Model serving using one line of code: `agents.deploy()`.\n",
    "\n",
    "The resulting Model Serving endpoint:\n",
    "- Is connected to the review app, which is a lightweight chat UI that can be shared with any user in your company, even if they don't have Databricks workspace access\n",
    "- Is integrated with AI Gateway so every request and response and its accompanying MLflow trace and user feedback is stored in an Inference Table\n",
    "\n",
    "Optionally, you can turn on Agent Evaluation’s monitoring capabilities, which are unified with the offline experience used above, and get a ready-to-go dashboard that runs judges on a sample of the traffic.\n",
    "\n",
    "<img src=\"https://docs.databricks.com/_static/images/generative-ai/synth-evals/review-app-optimized.gif\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca3749b2-6728-4aa3-aa8a-64b04b0a0b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "import mlflow\n",
    "\n",
    "# Connect to the Unity Catalog model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Configure UC model location\n",
    "UC_MODEL_NAME = f\"catalog.schema.db_docs_agent\"  # REPLACE WITH UC CATALOG/SCHEMA THAT YOU HAVE `CREATE MODEL` permissions in\n",
    "assert (\n",
    "    UC_MODEL_NAME != \"catalog.schema.db_docs_agent\"\n",
    "), \"Please replace 'catalog.schema.db_docs_agent' with your actual UC catalog and schema.\"\n",
    "\n",
    "# Register the Llama 70b version to Unity Catalog\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=model_info_llama_70b.model_uri, name=UC_MODEL_NAME\n",
    ")\n",
    "# Deploy to enable the review app and create an API endpoint\n",
    "deployment_info = agents.deploy(\n",
    "    model_name=UC_MODEL_NAME, model_version=uc_registered_model_info.version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "161747bb-154f-4ab4-b5d2-f1401f9649ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 5. Deploy to production and monitor\n",
    "\n",
    "**Challenges addressed**\n",
    "- How do I host my agent as a production ready, scalable service?\n",
    "- How do I execute tool code securely and ensure it respects my governance policies?\n",
    "- How do I enable telemetry or observability in development and production?\n",
    "- How do I monitor my agent’s quality at-scale in production? How do I quickly investigate and fix any quality issues?\n",
    "\n",
    "With Agent Framework, production deployment is the same for pre-production and production - you already have a highly scalable REST API that can be intergated in your application. This API provides an endpoint to get agent responses and to pass back user feedback so you can use that feedback to improve quality.\n",
    "\n",
    "To learn more about how monitoring works (in summary, Databricks has adapted a version of the above UIs and LLM judges for monitoring), read the documentation ([AWS](https://docs.databricks.com/en/generative-ai/agent-evaluation/evaluating-production-traffic.html) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-evaluation/evaluating-production-traffic)) or watch this [2 minute video](https://www.youtube.com/watch?v=ldAzmKkvQTU)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "10-minute-mosaic-ai-agent-demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}